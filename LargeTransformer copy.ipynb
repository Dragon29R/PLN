{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060f2673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8002\n",
      "7674\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from models import columns,vectorize_data\n",
    "from resultsAnalyse import drawConfusionMatrix\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "def filter_inadequada(example):\n",
    "    return example[\"INADEQUADA\"] == 0\n",
    "\n",
    "ds = load_dataset(\"higopires/RePro-categories-multilabel\")\n",
    "print(len(ds[\"train\"]))\n",
    "#remove INADEQUADA examples\n",
    "ds = ds.filter(filter_inadequada)\n",
    "#remove only keep the firs 1000 examples\n",
    "#ds[\"train\"] = ds[\"train\"].filter(lambda example, idx: idx < 1000, with_indices=True)\n",
    "#ds[\"test\"] = ds[\"test\"].filter(lambda example, idx: idx < 1000, with_indices=True)\n",
    "#ds[\"validation\"] = ds[\"validation\"].filter(lambda example, idx: idx < 1000, with_indices=True)\n",
    "print(len(ds[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67034519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to probabilities and then to binary predictions\n",
    "    predictions = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    print(f\"DEBUG: Original logits shape: {logits.shape}\")\n",
    "    print(f\"DEBUG: Original labels shape: {labels.shape}\")\n",
    "    # Calculate sample-wise F1 score\n",
    "    f1 = f1_score(labels, predictions, average='micro', zero_division=0)\n",
    "    \n",
    "    return {'f1_micro': float(f1)}\n",
    "model_name = \"neuralmind/bert-large-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def preprocess_function(sample):\n",
    "    # Tokenize text\n",
    "    tokenized = tokenizer(sample[\"review_text\"], truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    # Convert binary label columns to a list (e.g., [1, 0, 1, 0, 0, 0])\n",
    "    labels = []\n",
    "    for i in range(len(sample[\"review_text\"])):\n",
    "        label_row = [\n",
    "            float(sample[\"ENTREGA\"][i]),\n",
    "            float(sample[\"OUTROS\"][i]),\n",
    "            float(sample[\"PRODUTO\"][i]),\n",
    "            float(sample[\"CONDICOESDERECEBIMENTO\"][i]),\n",
    "            float(sample[\"ANUNCIO\"][i])\n",
    "        ]\n",
    "        labels.append(label_row)\n",
    "    \n",
    "    tokenized[\"labels\"] = torch.tensor(labels, dtype=torch.float)\n",
    "    return tokenized\n",
    "#check if output layer has 5 outputs\n",
    "#print(model.classifier.out_features)\n",
    "#model.classifier.out_features = 5  # Explicitly ensure final layer has 5 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7266c0",
   "metadata": {},
   "source": [
    "Add the LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c04da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-large-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#model = get_peft_model(model, peft_config)\n",
    "#model.print_trainable_parameters()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5,problem_type=\"multi_label_classification\",ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b4fb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "tokenized_dataset = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "126b3e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_3676\\1080485897.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resultsTransformer\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_micro\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f81e229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15350' max='15350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15350/15350 3:48:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.123039</td>\n",
       "      <td>0.937418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.122544</td>\n",
       "      <td>0.942904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.143370</td>\n",
       "      <td>0.944990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.148122</td>\n",
       "      <td>0.941253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.170081</td>\n",
       "      <td>0.940519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.174661</td>\n",
       "      <td>0.945264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.195626</td>\n",
       "      <td>0.942904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.205183</td>\n",
       "      <td>0.940289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.211926</td>\n",
       "      <td>0.940333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.208854</td>\n",
       "      <td>0.942251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Original logits shape: (952, 5)\n",
      "DEBUG: Original labels shape: (952, 5)\n",
      "DEBUG: Original logits shape: (952, 5)\n",
      "DEBUG: Original labels shape: (952, 5)\n",
      "DEBUG: Original logits shape: (952, 5)\n",
      "DEBUG: Original labels shape: (952, 5)\n",
      "DEBUG: Original logits shape: (952, 5)\n",
      "DEBUG: Original labels shape: (952, 5)\n",
      "DEBUG: Original logits shape: (952, 5)\n",
      "DEBUG: Original labels shape: (952, 5)\n",
      "DEBUG: Original logits shape: (952, 5)\n",
      "DEBUG: Original labels shape: (952, 5)\n",
      "DEBUG: Original logits shape: (952, 5)\n",
      "DEBUG: Original labels shape: (952, 5)\n",
      "DEBUG: Original logits shape: (952, 5)\n",
      "DEBUG: Original labels shape: (952, 5)\n",
      "DEBUG: Original logits shape: (952, 5)\n",
      "DEBUG: Original labels shape: (952, 5)\n",
      "DEBUG: Original logits shape: (952, 5)\n",
      "DEBUG: Original labels shape: (952, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15350, training_loss=0.04283255798420611, metrics={'train_runtime': 13686.0571, 'train_samples_per_second': 5.607, 'train_steps_per_second': 1.122, 'total_flos': 6.467711084040882e+16, 'train_loss': 0.04283255798420611, 'epoch': 10.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787f75f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Original logits shape: (966, 5)\n",
      "DEBUG: Original labels shape: (966, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.16487807035446167,\n",
       " 'test_f1_micro': 0.9478487614080834,\n",
       " 'test_runtime': 43.4037,\n",
       " 'test_samples_per_second': 22.256,\n",
       " 'test_steps_per_second': 4.47}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = trainer.predict(tokenized_dataset[\"test\"])\n",
    "y_pred.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cffcd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"Large_Bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951e7f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "del model  # Remove Python reference\n",
    "del trainer\n",
    "torch.cuda.empty_cache()  # Clear GPU memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
