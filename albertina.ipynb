{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3691b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomas\\.conda\\envs\\PLN\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8002\n",
      "7674\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "#from resultsAnalyse import drawConfusionMatrix\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "def filter_inadequada(example):\n",
    "    return example[\"INADEQUADA\"] == 0\n",
    "\n",
    "ds = load_dataset(\"higopires/RePro-categories-multilabel\")\n",
    "print(len(ds[\"train\"]))\n",
    "#remove INADEQUADA examples\n",
    "ds = ds.filter(filter_inadequada)\n",
    "#remove only keep the firs 1000 examples\n",
    "#ds[\"train\"] = ds[\"train\"].filter(lambda example, idx: idx < 1000, with_indices=True)\n",
    "#ds[\"test\"] = ds[\"test\"].filter(lambda example, idx: idx < 1000, with_indices=True)\n",
    "#ds[\"validation\"] = ds[\"validation\"].filter(lambda example, idx: idx < 1000, with_indices=True)\n",
    "print(len(ds[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c48fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to probabilities and then to binary predictions\n",
    "    predictions = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    print(f\"DEBUG: Original logits shape: {logits.shape}\")\n",
    "    print(f\"DEBUG: Original labels shape: {labels.shape}\")\n",
    "    # Calculate sample-wise F1 score\n",
    "    f1 = f1_score(labels, predictions, average='micro', zero_division=0)\n",
    "    acuracy = accuracy_score(labels, predictions, average='micro', zero_division=0)\n",
    "    precision = precision_score(labels, predictions, average='micro', zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average='micro', zero_division=0)\n",
    "    return {'f1_micro': float(f1), 'accuracy': float(acuracy), 'precision': float(precision), 'recall': float(recall)}\n",
    "model_name = \"PORTULAN/albertina-100m-portuguese-ptbr-encoder\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def preprocess_function(sample):\n",
    "    # Tokenize text\n",
    "    tokenized = tokenizer(sample[\"review_text\"], truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    # Convert binary label columns to a list (e.g., [1, 0, 1, 0, 0, 0])\n",
    "    labels = []\n",
    "    for i in range(len(sample[\"review_text\"])):\n",
    "        label_row = [\n",
    "            float(sample[\"ENTREGA\"][i]),\n",
    "            float(sample[\"OUTROS\"][i]),\n",
    "            float(sample[\"PRODUTO\"][i]),\n",
    "            float(sample[\"CONDICOESDERECEBIMENTO\"][i]),\n",
    "            float(sample[\"ANUNCIO\"][i])\n",
    "        ]\n",
    "        labels.append(label_row)\n",
    "    \n",
    "    tokenized[\"labels\"] = torch.tensor(labels, dtype=torch.float)\n",
    "    return tokenized\n",
    "#check if output layer has 5 outputs\n",
    "#print(model.classifier.out_features)\n",
    "#model.classifier.out_features = 5  # Explicitly ensure final layer has 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b94a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at PORTULAN/albertina-100m-portuguese-ptbr-encoder and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5,problem_type=\"multi_label_classification\",ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20794e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7674/7674 [00:01<00:00, 5748.73 examples/s]\n",
      "Map: 100%|██████████| 952/952 [00:00<00:00, 6060.36 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 6218.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "tokenized_dataset = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22964c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_13576\\2495128032.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resultsAlbertina\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_micro\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f214f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='15350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    4/15350 00:14 < 30:55:02, 0.14 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36046355",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = trainer.predict(tokenized_dataset[\"test\"])\n",
    "y_pred.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"alberta_base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
