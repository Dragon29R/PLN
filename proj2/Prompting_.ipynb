{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8201129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f29cbe",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4587019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8002\n",
      "7674\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "def filter_inadequada(example):\n",
    "    return example[\"INADEQUADA\"] == 0\n",
    "\n",
    "ds = load_dataset(\"higopires/RePro-categories-multilabel\")\n",
    "print(len(ds[\"train\"]))\n",
    "#remove INADEQUADA examples\n",
    "ds = ds.filter(filter_inadequada)\n",
    "#remove only keep the firs 1000 examples\n",
    "#ds[\"train\"] = ds[\"train\"].filter(lambda example, idx: idx < 20, with_indices=True)\n",
    "#ds[\"test\"] = ds[\"test\"].filter(lambda example, idx: idx < 5, with_indices=True)\n",
    "#ds[\"validation\"] = ds[\"validation\"].filter(lambda example, idx: idx < 5, with_indices=True)\n",
    "print(len(ds[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f7ac0",
   "metadata": {},
   "source": [
    "# Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1f436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt):\n",
    "    prompt = \"\"\"Classifique o texto fornecido com uma ou mais das seguintes características: ENTREGA, OUTROS, PRODUTO, CONDICOESDERECEBIMENTO, ANUNCIO. Forneça a resposta como uma lista de características separadas por vírgulas.\n",
    "\n",
    "    ### Exemplos:\n",
    "\n",
    "    Texto: \"A entrega foi super rápida, chegou em 2 dias!\"\n",
    "    Características: ENTREGA\n",
    "\n",
    "    Texto: \"O smartphone tem uma câmera incrível e a bateria dura o dia todo.\"\n",
    "    Características: PRODUTO\n",
    "\n",
    "    Texto: \"O anúncio mencionava frete grátis, mas fui cobrado no final.\"\n",
    "    Características: ANUNCIO, ENTREGA\n",
    "\n",
    "    Texto: \"Não gostei da experiência geral, muitas interrupções.\"\n",
    "    Características: OUTROS\n",
    "\n",
    "    Texto: \"O produto chegou em boas condições e a entrega foi rapida.\"\n",
    "    Características: ENTREGA, PRODUTO, CONDICOESDERECEBIMENTO\n",
    "\n",
    "    ### Novo Texto:\n",
    "\n",
    "    Texto: \"{review_text}\"\n",
    "    Características:\n",
    "    \"\"\"\n",
    "\n",
    "    # Example usage\n",
    "    review_text_example = prompt\n",
    "    formatted_prompt = prompt.format(review_text=review_text_example)\n",
    "\n",
    "    sequences = pipe(\n",
    "        formatted_prompt,\n",
    "        max_new_tokens=50,\n",
    "        return_full_text = False,\n",
    "    )\n",
    "    result = [0,0,0,0,0]\n",
    "    for seq in sequences:\n",
    "        generatedText = seq[\"generated_text\"]\n",
    "        if \"ENTREGA\" in generatedText:\n",
    "            result[0]=1\n",
    "        if \"OUTROS\" in generatedText:\n",
    "            result[1]=1\n",
    "        if \"PRODUTO\" in generatedText:\n",
    "            result[2]=1\n",
    "        if \"CONDICOESDERECEBIMENTO\" in generatedText:\n",
    "            result[3]=1\n",
    "        if \"ANUNCIO\" in generatedText:\n",
    "            result[4]=1\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937283a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 0 of 966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 100 of 966\n",
      "Processing example 200 of 966\n",
      "Processing example 300 of 966\n",
      "Processing example 400 of 966\n",
      "Processing example 500 of 966\n",
      "Processing example 600 of 966\n",
      "Processing example 700 of 966\n",
      "Processing example 800 of 966\n",
      "Processing example 900 of 966\n"
     ]
    }
   ],
   "source": [
    "Predictions = []\n",
    "GroundTruth = []\n",
    "for i in range(len(ds[\"test\"])):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing example {i} of {len(ds['test'])}\")\n",
    "    sample = ds[\"test\"][i]\n",
    "    review_text = [\"review_text\"]\n",
    "    result = run_prompt(review_text)\n",
    "    labels = [sample[\"ENTREGA\"],sample[\"OUTROS\"],sample[\"PRODUTO\"],sample[\"CONDICOESDERECEBIMENTO\"],sample[\"ANUNCIO\"]]\n",
    "    #print(f\"Result: {result}\")\n",
    "    #print(f\"Labels: {labels}\")\n",
    "    Predictions.append(result)\n",
    "    GroundTruth.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371a2e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_micro': 0.4788048552754435, 'precision': 0.33481326717158527, 'recall': 0.8401048492791612}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "def compute_metrics(predictions,labels):\n",
    "    # Calculate sample-wise F1 score\n",
    "    f1 = f1_score(labels, predictions, average='micro', zero_division=0)\n",
    "    precision = precision_score(labels, predictions, average='micro', zero_division=0)\n",
    "\n",
    "    recall = recall_score(labels, predictions, average='micro', zero_division=0)\n",
    "    return {'f1_micro': float(f1), 'precision': float(precision), 'recall': float(recall)}\n",
    "print(compute_metrics(Predictions,GroundTruth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da96e9",
   "metadata": {},
   "source": [
    "# Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff28a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt):\n",
    "    prompt = \"\"\"Classifique o texto fornecido com uma ou mais das seguintes características: ENTREGA, OUTROS, PRODUTO, CONDICOESDERECEBIMENTO, ANUNCIO. Forneça a resposta como uma lista de características separadas por vírgulas.\n",
    "\n",
    "    Texto: \"{review_text}\"\n",
    "    Características:\n",
    "    \"\"\"\n",
    "\n",
    "    # Example usage\n",
    "    review_text_example = prompt\n",
    "    formatted_prompt = prompt.format(review_text=review_text_example)\n",
    "\n",
    "    sequences = pipe(\n",
    "        formatted_prompt,\n",
    "        max_new_tokens=50,\n",
    "        return_full_text = False,\n",
    "    )\n",
    "    result = [0,0,0,0,0]\n",
    "    for seq in sequences:\n",
    "        generatedText = seq[\"generated_text\"]\n",
    "        if \"ENTREGA\" in generatedText:\n",
    "            result[0]=1\n",
    "        if \"OUTROS\" in generatedText:\n",
    "            result[1]=1\n",
    "        if \"PRODUTO\" in generatedText:\n",
    "            result[2]=1\n",
    "        if \"CONDICOESDERECEBIMENTO\" in generatedText:\n",
    "            result[3]=1\n",
    "        if \"ANUNCIO\" in generatedText:\n",
    "            result[4]=1\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 0 of 966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 100 of 966\n",
      "Processing example 200 of 966\n",
      "Processing example 300 of 966\n",
      "Processing example 400 of 966\n",
      "Processing example 500 of 966\n",
      "Processing example 600 of 966\n",
      "Processing example 700 of 966\n",
      "Processing example 800 of 966\n",
      "Processing example 900 of 966\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29340\\1774451016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mPredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mGroundTruth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGroundTruth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "Predictions = []\n",
    "GroundTruth = []\n",
    "for i in range(len(ds[\"test\"])):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing example {i} of {len(ds['test'])}\")\n",
    "    sample = ds[\"test\"][i]\n",
    "    review_text = [\"review_text\"]\n",
    "    result = run_prompt(review_text)\n",
    "    labels = [sample[\"ENTREGA\"],sample[\"OUTROS\"],sample[\"PRODUTO\"],sample[\"CONDICOESDERECEBIMENTO\"],sample[\"ANUNCIO\"]]\n",
    "    #print(f\"Result: {result}\")\n",
    "    #print(f\"Labels: {labels}\")\n",
    "    Predictions.append(result)\n",
    "    GroundTruth.append(labels)\n",
    "def compute_metrics(predictions,labels):\n",
    "    # Calculate sample-wise F1 score\n",
    "    f1 = f1_score(labels, predictions, average='micro', zero_division=0)\n",
    "    precision = precision_score(labels, predictions, average='micro', zero_division=0)\n",
    "\n",
    "    recall = recall_score(labels, predictions, average='micro', zero_division=0)\n",
    "    return {'f1_micro': float(f1), 'precision': float(precision), 'recall': float(recall)}\n",
    "print(compute_metrics(Predictions,GroundTruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d58a07ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_micro': 0.4792578330124279, 'precision': 0.3269644136613327, 'recall': 0.8971166448230669}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "def compute_metrics(predictions,labels):\n",
    "    # Calculate sample-wise F1 score\n",
    "    f1 = f1_score(labels, predictions, average='micro', zero_division=0)\n",
    "    precision = precision_score(labels, predictions, average='micro', zero_division=0)\n",
    "\n",
    "    recall = recall_score(labels, predictions, average='micro', zero_division=0)\n",
    "    return {'f1_micro': float(f1), 'precision': float(precision), 'recall': float(recall)}\n",
    "print(compute_metrics(Predictions,GroundTruth))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
