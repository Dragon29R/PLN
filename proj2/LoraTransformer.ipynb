{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060f2673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8002\n",
      "7674\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from models import columns,vectorize_data\n",
    "from resultsAnalyse import drawConfusionMatrix\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "def filter_inadequada(example):\n",
    "    return example[\"INADEQUADA\"] == 0\n",
    "\n",
    "ds = load_dataset(\"higopires/RePro-categories-multilabel\")\n",
    "print(len(ds[\"train\"]))\n",
    "#remove INADEQUADA examples\n",
    "ds = ds.filter(filter_inadequada)\n",
    "#remove only keep the firs 1000 examples\n",
    "#ds[\"train\"] = ds[\"train\"].filter(lambda example, idx: idx < 20, with_indices=True)\n",
    "#ds[\"test\"] = ds[\"test\"].filter(lambda example, idx: idx < 5, with_indices=True)\n",
    "#ds[\"validation\"] = ds[\"validation\"].filter(lambda example, idx: idx < 5, with_indices=True)\n",
    "print(len(ds[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67034519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to probabilities and then to binary predictions\n",
    "    predictions = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    #print(f\"DEBUG: Original logits shape: {logits.shape}\")\n",
    "    #print(f\"DEBUG: Original labels shape: {labels.shape}\")\n",
    "    # Calculate sample-wise F1 score\n",
    "    f1 = f1_score(labels, predictions, average='micro', zero_division=0)\n",
    "    \n",
    "    return {'f1_micro': float(f1)}\n",
    "#model_name = \"neuralmind/bert-large-portuguese-cased\"\n",
    "model_name = \"neuralmind/bert-large-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def preprocess_function(sample):\n",
    "    # Tokenize text\n",
    "    tokenized = tokenizer(sample[\"review_text\"], truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    # Convert binary label columns to a list (e.g., [1, 0, 1, 0, 0, 0])\n",
    "    labels = []\n",
    "    for i in range(len(sample[\"review_text\"])):\n",
    "        label_row = [\n",
    "            float(sample[\"ENTREGA\"][i]),\n",
    "            float(sample[\"OUTROS\"][i]),\n",
    "            float(sample[\"PRODUTO\"][i]),\n",
    "            float(sample[\"CONDICOESDERECEBIMENTO\"][i]),\n",
    "            float(sample[\"ANUNCIO\"][i])\n",
    "        ]\n",
    "        labels.append(label_row)\n",
    "    \n",
    "    tokenized[\"labels\"] = torch.tensor(labels, dtype=torch.float)\n",
    "    return tokenized\n",
    "#check if output layer has 5 outputs\n",
    "#print(model.classifier.out_features)\n",
    "#model.classifier.out_features = 5  # Explicitly ensure final layer has 5 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7266c0",
   "metadata": {},
   "source": [
    "Add the LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0b1db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-large-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5,problem_type=\"multi_label_classification\",ignore_mismatched_sizes=True)\n",
    "#print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dad37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS #for sequence classification\n",
    "                         , inference_mode=False #for inference mode\n",
    "                         , r=64, lora_alpha=32, lora_dropout=0.1) #for dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c04da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,296,581 || all params: 340,698,122 || trainable%: 1.8481\n",
      "trainable params: 6,301,706 || all params: 340,698,122 || trainable%: 1.8496\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "#unfreeze the classifier layer\n",
    "for param in model.base_model.model.classifier.parameters():\n",
    "   param.requires_grad = True  # Make this layer trainable\n",
    "model.print_trainable_parameters()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4fb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "tokenized_dataset = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "126b3e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_23936\\4010896594.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resultsTransformer\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_micro\",\n",
    "\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f81e229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9600' max='9600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9600/9600 2:40:45, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.540200</td>\n",
       "      <td>0.390997</td>\n",
       "      <td>0.654088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>0.284333</td>\n",
       "      <td>0.806185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.286900</td>\n",
       "      <td>0.255257</td>\n",
       "      <td>0.820751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>0.233096</td>\n",
       "      <td>0.839559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.240300</td>\n",
       "      <td>0.218723</td>\n",
       "      <td>0.852900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.229700</td>\n",
       "      <td>0.205758</td>\n",
       "      <td>0.859551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.219700</td>\n",
       "      <td>0.197811</td>\n",
       "      <td>0.868990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.191978</td>\n",
       "      <td>0.875130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>0.188551</td>\n",
       "      <td>0.881426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.204200</td>\n",
       "      <td>0.187515</td>\n",
       "      <td>0.879973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9600, training_loss=0.2677590525150299, metrics={'train_runtime': 9646.4059, 'train_samples_per_second': 7.955, 'train_steps_per_second': 0.995, 'total_flos': 6.898756224996792e+16, 'train_loss': 0.2677590525150299, 'epoch': 10.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "787f75f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.20323899388313293,\n",
       " 'test_f1_micro': 0.8645690834473324,\n",
       " 'test_runtime': 43.9664,\n",
       " 'test_samples_per_second': 21.971,\n",
       " 'test_steps_per_second': 2.752}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = trainer.predict(tokenized_dataset[\"test\"])\n",
    "y_pred.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cffcd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"lora_Portuguese_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a951e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model  # Remove Python reference\n",
    "torch.cuda.empty_cache()  # Clear GPU memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
